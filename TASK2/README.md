# Лабораторная работа: Параллельное умножение матрицы на вектор с использованием MPI (строчное, столбцовое и блочное распределение)

## Цель работы

Реализовать и исследовать несколько вариантов параллельного алгоритма умножения матрицы на вектор с использованием библиотеки **MPI**:

- построчное распределение матрицы (**rows**),
- постолбцовое распределение (**cols**),
- двумерное блочное распределение (**blocks**).

Необходимо сравнить последовательное и параллельное выполнение, оценить максимальное время выполнения среди процессов, ускорение и эффективность в зависимости от размера матрицы **N** и числа процессов **P**.

---

## Описание реализованных алгоритмов

В программе представлены три варианта параллельного умножения матрицы на вектор. Во всех алгоритмах матрица **A** размера *N × N* и вектор **v** размерности *N* распределяются между процессами MPI, и каждый процесс вычисляет свою часть результата.

### 1. Строчное распределение (rows)

Матрица разбивается **по строкам**, и каждый процесс получает:

- подматрицу **Aᵖ** размером *(N / P) × N*,  
- общий вектор **v**, передаваемый через `MPI_Bcast`.

Процесс вычисляет часть результата:

**yᵖ = Aᵖ × v**

Части результирующего вектора собираются в корневом процессе через `MPI_Gatherv`.

**Плюсы:**
- простая реализация.

**Минусы:**
- большие пересылки строк матрицы,
- слабая масштабируемость при большом числе процессов.

---

### 2. Столбцовое распределение (cols)

Матрица делится **по столбцам**, а каждый процесс получает:

- набор столбцов **Aᵖ** (размером *N × (N / P)*),
- соответствующую часть вектора **vᵖ**.

Каждый процесс вычисляет частичный вклад:

**yᵖ[i] += A[i, j] × v[j]**

Итоговый вектор собирается через:

`MPI_Reduce(..., MPI_SUM)`.

**Плюсы:**
- равномерное распределение вычислений,
- уменьшение объёма пересылаемых данных.

**Минусы:**
- требуется переструктурирование столбцов матрицы.

---

### 3. Блочное двумерное распределение (blocks)

Создаётся **двумерная декартова топология процессов**:

- сетка *p_r × p_c*,
- матрица делится на блоки Aᵢⱼ,
- вектор делится между процессами столбцов сетки.

Используются подкоммуникаторы:

- `row_comm` — процессы в строке блока,  
- `col_comm` — процессы в столбце блока.

Каждый процесс вычисляет:

**yᵖ = Aᵖ × vᵖ**

Результаты объединяются в два этапа:

1. Суммирование внутри строки (`MPI_Reduce`).
2. Сборка глобального вектора в процессе 0.

**Плюсы:**
- минимизация коммуникаций,
- лучшая масштабируемость.

**Минусы:**
- наибольшая сложность реализации.

---

## Роль MPI

В работе используются:

- `MPI_Init`, `MPI_Finalize` — управление окружением MPI.  
- `MPI_Comm_rank`, `MPI_Comm_size` — определение номера и количества процессов.  
- `MPI_Scatterv`, `MPI_Gatherv` — распределение и сбор строк (rows).  
- `MPI_Send`, `MPI_Recv` — рассылка блоков и столбцов (cols, blocks).  
- `MPI_Reduce` — суммирование частичных результатов, вычисление максимального времени.  
- `MPI_Bcast` — широковещательная рассылка вектора.  
- `MPI_Cart_create` — создание 2D-топологии для блочного алгоритма.

MPI обеспечивает распределение вычислений и взаимодействие процессов без центрального управления.

---

## Анализ графиков

*(Графики строятся на основе данных, записанных программой в файл `result.csv`.)*

### 1. Время выполнения (Tₘₐₓ vs N)

Время выполнения растёт с увеличением размера матрицы **N**, что соответствует вычислительной сложности O(N²).  
Увеличение числа процессов **P** уменьшает время, особенно на больших матрицах.

Характеристики:

- **rows** — самая большая задержка при пересылках.  
- **cols** — более стабильное время при росте числа процессов.  
- **blocks** — минимальное время среди трёх алгоритмов.

---

### 2. Ускорение (Speedup S vs P)

Ускорение:

**S = T₁ / Tₚ**

- при малых N ускорение незначительно — коммуникации доминируют,  
- при больших N ускорение растёт, особенно у блочного алгоритма.

Наилучшая масштабируемость:

1. **blocks**  
2. **cols**  
3. **rows**

---

### 3. Ускорение (Speedup S vs N)

При фиксированном числе процессов ускорение растёт с увеличением N:

- вычисления начинают доминировать над пересылками,  
- эффективность распределения нагрузки выше.

**blocks** показывает наиболее плавный рост.

---

### 4. Эффективность (E = S / P)

Эффективность снижается при увеличении числа процессов:

- у небольших матриц коммуникации составляют значительную долю времени,  
- у больших матриц эффективность растёт и дольше остаётся высокой.

Лучшая эффективность:

1. **blocks**,  
2. **cols**,  
3. **rows**.

---

## Выводы

- Реализованы три параллельных алгоритма умножения матрицы на вектор: **rows**, **cols**, **blocks**.
- Проведён анализ производительности при разных размерах матрицы и числе процессов.
- Алгоритм **blocks** показал лучшую масштабируемость и минимальные коммуникационные затраты.
- Алгоритм **rows** — самый простой в реализации, но худший по производительности при больших P.
- Алгоритм **cols** занимает промежуточное положение.
- Полученные результаты подтверждают эффективность параллельных вычислений для больших задач.
- Файл `result.csv` позволяет визуализировать время выполнения, ускорение и эффективность.

---